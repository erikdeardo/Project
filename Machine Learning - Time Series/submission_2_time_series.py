# -*- coding: utf-8 -*-
"""Submission 2 Time Series.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OKGbFetGyJYu2fUoQ4lwXOnbz5EgRTJI
"""

import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf

df = pd.read_csv('/content/drive/My Drive/Dataset/jena_climate_2009_2016/jena_climate_2009_2016.csv')

print(df.isnull().sum())
df.head()

# membuat data menjadi per jam.
df = df[5::6]
df.head()

df_rho = df[['Date Time', 'rho (g/m**3)']]
df_rho

df_rho.describe()

# Mencari nilai 10% skala data dari kolom rho (g/m**3)

max_rho = df_rho['rho (g/m**3)'].max()
min_rho = df_rho['rho (g/m**3)'].min()


diff_rho = max_rho - min_rho

max_MAE = diff_rho*0.1

print('Nilai maksimal rho (g/m**3) : ', max_rho, '\nNilai minimal rho (g/m**3) : ', min_rho)
print('Nilai maksimal MAE (10% dari skala data (rho (g/m**3))): ',max_MAE,)

# Membagi data train (80%) data data validation (20%)

jml_data = len(df_rho)

rho_train = df_rho[0:int(jml_data*0.8)]
rho_val = df_rho[int(jml_data*0.8):]

print('Total data : ',jml_data)
print('Jumlah data train : ',len(rho_train))
print('Jumlah data validasi : ',len(rho_val))

# Plotting data train dan data validation

dates_train = rho_train['Date Time'].values
dates_val = rho_val['Date Time'].values

df_rho_train = rho_train['rho (g/m**3)'].values
df_rho_val = rho_val['rho (g/m**3)'].values

plt.figure(figsize=(15,5))

ax1 = plt.plot(dates_train,df_rho_train)
ax2 = plt.plot(dates_val,df_rho_val)

plt.title('rho (g/m**3)', fontsize=20)
plt.legend(['train', 'val'], loc='upper right')

plt.show(ax1)
plt.show(ax2)

# Merubah data menjadi format yang dapat diterima oleh model
def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[1:]))
    return ds.batch(batch_size).prefetch(1)

# Menambah fungsi callback saat train data

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')<max_MAE):
      print("\nMAE telah mencapai < 10% skala data!")
      self.model.stop_training = True
callbacks = myCallback()

# Membuat model

train_set = windowed_dataset(df_rho_train, window_size=24, batch_size=64, shuffle_buffer=1000)
val_set = windowed_dataset(df_rho_train, window_size=24, batch_size=64, shuffle_buffer=1000)

model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(100, return_sequences=True),
  tf.keras.layers.LSTM(64),
  tf.keras.layers.Dense(64, activation="relu"),
  tf.keras.layers.Dense(32, activation="relu"),
  tf.keras.layers.Dense(1),
])

# train data

optimizer = tf.keras.optimizers.SGD(lr=0.0001, momentum=0.95)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(train_set,epochs=10, validation_data=val_set, callbacks=[callbacks])

plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('MAE Model')
plt.ylabel('mae')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper right')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper right')
plt.show()

